[
  {
    "objectID": "manuscript/index.html",
    "href": "manuscript/index.html",
    "title": "Defining challenges and benchmarking open problems in Single-cell analysis",
    "section": "",
    "text": "Single-cell genomics has enabled the study of biological processes at an unprecedented scale and resolution (Plass et al. 2018; Cao et al. 2020; Montoro et al. 2018). These studies have been enabled by computational tools for the analysis of single-cell data, which has surpassed 1300 published algorithms (Zappia and Theis 2021). Evolving biological questions and technological development (“Method of the Year 2020: Spatially Resolved Transcriptomics” 2021; “Method of the Year 2019: Single-Cell Multimodal Omics” 2020) continuously pose new challenges for data analysts, which has led to the description of grand challenges for single-cell data science (Lähnemann et al. 2020). While the identification of such grand challenges in single-cell genomics can concentrate research into relevant areas, the current definitions of these challenges are qualitative in nature.\nClear definitions of open problems including quantifications of progress has driven innovation in machine learning research (Donoho 2017). Open challenges such as the Netflix prize (Bennett, Lanning, et al. 2007) and ImageNet (Deng et al. 2009) represent landmark problems that allow measurement of progress across decades. These challenges have provided a common vocabulary for innovation: image classification accuracy has improved from 50% to over 90% on ImageNet since 2011. Inspired by open challenges in machine learning, Critical Assessment of protein Structure Prediction (CASP), Recursion’s cellular image classification Kaggle competition, various DREAM challenges, and our recent multimodal data integration competition at NeurIPS 2021 (Malte D. Luecken et al. 2021a) have focused research efforts in computational biology disciplines such as protein structure prediction, cellular diagnostics, gene module prediction, and multimodal single-cell data integration. Particularly successful competitions such as CASP have built communities that accelerate research toward these goals.\nIn single-cell genomics, large-scale benchmarks (Saelens et al. 2019; Malte D. Luecken et al. 2021b; Soneson and Robinson 2018; Squair et al. 2021) have quantified open questions by proposing evaluation metrics and using these to compare existing methods. Yet, benchmarks inevitably age: newly developed tools that optimize the proposed benchmarking metrics cannot be compared in the same study; existing tools may perform differently on newly generated benchmarking datasets of better quality; and used metrics may not capture the aspects of method performance that a user is interested in. These challenges can only be addressed by community participation in benchmarking. To build such communities around quantified challenges, benchmarking studies would require frequent updating and open discussion on benchmarking datasets, proposed metrics, and selected methods.\nLearning from machine learning competitions, we propose a living benchmarking framework for single-cell genomics tools that follows the Common Task Framework (CTF) (Donoho 2017). The CTF has 3 criteria: (1) An easily available training dataset of sufficient complexity, (2) a set of competitors who submit methods for benchmarking, and (3) a set of metrics that are evaluated on test data to define progress on the task and thereby formally define it. When continuously updating such benchmarks, …\nWe have developed the Open Problems in Single-cell Analysis (Open Problems) framework, a platform that defines and measures progress towards open challenges in single-cell genomics following the CTF. Open Problems combines an open github repository with community-defined tasks, Github Actions testing workflows, a benchmarking pipeline using Nextflow (Tommaso et al. 2017) and Amazon Web Services, and a website to explore the results to build a living, community-based benchmarking project. Currently, Open Problems includes X defined tasks, on which Y datasets are used to evaluate Z methods using XY metrics. These tasks were defined by interactions between contributors, which has led to new benchmarking insights even when contributors had already published benchmarks on these tasks. Open Problems provides community-defined standards for progress in single-cell data science to enable decentralized method development towards a common goal."
  },
  {
    "objectID": "manuscript/index.html#an-infrastructure-for-living-benchmarks-in-single-cell-data-science",
    "href": "manuscript/index.html#an-infrastructure-for-living-benchmarks-in-single-cell-data-science",
    "title": "Defining challenges and benchmarking open problems in Single-cell analysis",
    "section": "An infrastructure for living benchmarks in Single-cell Data Science",
    "text": "An infrastructure for living benchmarks in Single-cell Data Science\nIn order to enable a truly living benchmark, we have designed a standardized and automated infrastructure which allows members of the single-cell community to contribute to Open Problems in a seamless manner. Each Open Problems task is comprised of datasets, methods, and metrics (Fig 1): datasets define both the input and the ground truth for a task, methods attempt to solve the task, and metrics evaluate the success of a method on a given dataset. Metrics are normalized between 0 and 1 based on the inclusion of “baseline” methods which are designed to emulate either random or perfect performance. Methods are then ranked on a per-dataset basis by the average normalized metric score and presented in a summary table on the Open Problems website (openproblems.bio).\nTo enable seamless community involvement in Open Problems, we have designed the infrastructure to take advantage of automated workflows through GitHub Actions, Nextflow[cite], and AWS Batch. When a community member adds a task, dataset, method, or metric, the new contributions are automatically tested on the cloud. When all tests pass and the new contribution is merged into the main repository, the results from the new contribution are automatically submitted to the Open Problems website. To maximize reproducibility, all code is run within Docker containers and all data is downloaded from public repositories, including figshare, GEO, and CELLxGENE (Megill et al. 2021). Task definitions, choices of metrics, and implementations of methods care discussed on our github repository (github.com/openproblems-bio/openproblems) and can be easily amended by pull requests which are reviewed by task leaders (who initially defined the task) and the core infrastructure team."
  },
  {
    "objectID": "manuscript/index.html#community-defined-open-problems-in-single-cell-analysis",
    "href": "manuscript/index.html#community-defined-open-problems-in-single-cell-analysis",
    "title": "Defining challenges and benchmarking open problems in Single-cell analysis",
    "section": "Community-defined open problems in Single-cell analysis",
    "text": "Community-defined open problems in Single-cell analysis\nBuilding on previous work defining open challenges in single-cell analysis (Lähnemann et al. 2020) and many independent benchmarking studies in single-cell genomics (Malte D. Luecken et al. 2021b; Soneson and Robinson 2018; Li et al. 2022; Hou et al. 2020; Raimundo, Vallot, and Vert 2020; X. Sun et al. 2022; S. Sun et al. 2019; Huang and Zhang 2021; Chazarra-Gil et al. 2021; Cantini et al. 2021; Cobos et al. 2020), we defined X Open Problems tasks (Fig 2a). While some tasks were directly transferred from published benchmarking papers (e.g., batch correction12), others were defined directly by method developers in the single-cell community (e.g., spatial decomposition). To exemplify how such a task is defined and the value that the task adds, we elaborate on the spatial deconvolution task."
  },
  {
    "objectID": "manuscript/index.html#proposed-structure",
    "href": "manuscript/index.html#proposed-structure",
    "title": "Defining challenges and benchmarking open problems in Single-cell analysis",
    "section": "Proposed structure",
    "text": "Proposed structure\n\nTask overview\nResults from a few tasks\n\n\nAll of the scIB benchmarking can be migrated into this framework (highly extensible)\nAddition of completely novel tasks (e.g. gene regulatory prediction)\nCommunity-driven benchmarking for emerging methodology (e.g. differential abundance, spatial decomposition)\nResults of the CZI jamboree\n\n\nFocus on spatial decomposition task\n\n\nExplain metrics, datasets, methods"
  },
  {
    "objectID": "manuscript/index.html#defined-open-problems-facilitatedrive-innovation-in-single-cell-data-science",
    "href": "manuscript/index.html#defined-open-problems-facilitatedrive-innovation-in-single-cell-data-science",
    "title": "Defining challenges and benchmarking open problems in Single-cell analysis",
    "section": "Defined open problems facilitate/drive innovation in single-cell data science",
    "text": "Defined open problems facilitate/drive innovation in single-cell data science\n\nReaching out to the ML community:\n\n\nNeurIPS competition built on Open Problems - Note: Community-informed re-evaluation of metrics (PMLR report)\n\n\nUsing framework to drive development of novel methods\nEnable method developers to include their methods in live benchmark and publish results\nBuild analysis pipelines from top performers across tasks"
  },
  {
    "objectID": "results/cell_cell_communication_ligand_target/index.html",
    "href": "results/cell_cell_communication_ligand_target/index.html",
    "title": "Cell-Cell Communication Inference (Ligand-Target)",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/cell_cell_communication_ligand_target/index.html#compute-scaling-factors",
    "href": "results/cell_cell_communication_ligand_target/index.html#compute-scaling-factors",
    "title": "Cell-Cell Communication Inference (Ligand-Target)",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/cell_cell_communication_ligand_target/index.html#overview",
    "href": "results/cell_cell_communication_ligand_target/index.html#overview",
    "title": "Cell-Cell Communication Inference (Ligand-Target)",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/cell_cell_communication_ligand_target/index.html#quality-control",
    "href": "results/cell_cell_communication_ligand_target/index.html#quality-control",
    "title": "Cell-Cell Communication Inference (Ligand-Target)",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    auprc \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    odds_ratio \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    auprc \n    -1 \n    -0.017 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    odds_ratio \n    -1 \n    -0.269 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    28 \n    28.000 \n    28.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    tnbc_data \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cellphonedb_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cellphonedb_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    connectome_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    connectome_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liana_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liana_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    logfc_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    logfc_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    natmi_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    natmi_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_events \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    sca_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    sca_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    true_events \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    auprc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    odds_ratio \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/cell_cell_communication_ligand_target/index.html#raw-data",
    "href": "results/cell_cell_communication_ligand_target/index.html#raw-data",
    "title": "Cell-Cell Communication Inference (Ligand-Target)",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/label_projection/index.html",
    "href": "results/label_projection/index.html",
    "title": "Label Projection",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/label_projection/index.html#compute-scaling-factors",
    "href": "results/label_projection/index.html#compute-scaling-factors",
    "title": "Label Projection",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/label_projection/index.html#overview",
    "href": "results/label_projection/index.html#overview",
    "title": "Label Projection",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/label_projection/index.html#quality-control",
    "href": "results/label_projection/index.html#quality-control",
    "title": "Label Projection",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    accuracy \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    f1 \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    f1_macro \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    accuracy \n    -1 \n    -0.031 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    f1_macro \n    -1 \n    -0.037 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    f1 \n    -1 \n    -0.051 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    384 \n    384.000 \n    384.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    cengen_batch \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    cengen_random \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_batch \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_random \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_random_label_noise \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    tabula_muris_senis_lung_random \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    zebrafish_labels \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    zebrafish_random \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    knn_classifier_log_cpm \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    knn_classifier_scran \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    logistic_regression_log_cpm \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    logistic_regression_scran \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    majority_vote \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mlp_log_cpm \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mlp_scran \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_labels \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanvi_all_genes \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanvi_hvg \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scarches_scanvi_all_genes \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scarches_scanvi_hvg \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    seurat \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    true_labels \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    xgboost_log_cpm \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    xgboost_scran \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    accuracy \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    f1 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    f1_macro \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/label_projection/index.html#raw-data",
    "href": "results/label_projection/index.html#raw-data",
    "title": "Label Projection",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/batch_integration_graph/index.html",
    "href": "results/batch_integration_graph/index.html",
    "title": "Batch integration graph",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/batch_integration_graph/index.html#compute-scaling-factors",
    "href": "results/batch_integration_graph/index.html#compute-scaling-factors",
    "title": "Batch integration graph",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/batch_integration_graph/index.html#overview",
    "href": "results/batch_integration_graph/index.html#overview",
    "title": "Batch integration graph",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/batch_integration_graph/index.html#quality-control",
    "href": "results/batch_integration_graph/index.html#quality-control",
    "title": "Batch integration graph",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    isolated_labels_f1 \n    -1 \n    4.28 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    ari \n    -1 \n    1.00 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    graph_connectivity \n    -1 \n    1.00 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    nmi \n    -1 \n    1.00 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    ari \n    -1 \n    0.00 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    graph_connectivity \n    -1 \n    0.00 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    isolated_labels_f1 \n    -1 \n    0.00 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    nmi \n    -1 \n    0.00 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    360 \n    360.00 \n    360.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    immune_batch \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_batch \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    batch_random_integration \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    bbknn_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    bbknn_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    bbknn_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    bbknn_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    celltype_random_graph \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    celltype_random_integration \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liger_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liger_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    no_integration \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_integration \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scalex_full \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scalex_hvg \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_full_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_hvg_scaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanvi_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanvi_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scvi_full_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scvi_hvg_unscaled \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    ari \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    graph_connectivity \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    isolated_labels_f1 \n    0 \n    0.00 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    nmi \n    0 \n    0.00 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/batch_integration_graph/index.html#raw-data",
    "href": "results/batch_integration_graph/index.html#raw-data",
    "title": "Batch integration graph",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/multimodal_data_integration/index.html",
    "href": "results/multimodal_data_integration/index.html",
    "title": "Multimodal Data Integration",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/multimodal_data_integration/index.html#compute-scaling-factors",
    "href": "results/multimodal_data_integration/index.html#compute-scaling-factors",
    "title": "Multimodal Data Integration",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/multimodal_data_integration/index.html#overview",
    "href": "results/multimodal_data_integration/index.html#overview",
    "title": "Multimodal Data Integration",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/multimodal_data_integration/index.html#quality-control",
    "href": "results/multimodal_data_integration/index.html#quality-control",
    "title": "Multimodal Data Integration",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    knn_auc \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    mse \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    knn_auc \n    -1 \n    -0.014 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    mse \n    -1 \n    -0.029 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    42 \n    42.000 \n    42.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    citeseq_cbmc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    scicar_cell_lines \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    scicar_mouse_kidney \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmonic_alignment_log_scran_pooling \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmonic_alignment_sqrt_cpm \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_log_cpm \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_log_scran_pooling \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    procrustes \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_features \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    true_features \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    knn_auc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    mse \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/multimodal_data_integration/index.html#raw-data",
    "href": "results/multimodal_data_integration/index.html#raw-data",
    "title": "Multimodal Data Integration",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/batch_integration_embed/index.html",
    "href": "results/batch_integration_embed/index.html",
    "title": "Batch integration embed",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)\n\nWarning: Removed 78 rows containing missing values (geom_point)."
  },
  {
    "objectID": "results/batch_integration_embed/index.html#compute-scaling-factors",
    "href": "results/batch_integration_embed/index.html#compute-scaling-factors",
    "title": "Batch integration embed",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/batch_integration_embed/index.html#overview",
    "href": "results/batch_integration_embed/index.html#overview",
    "title": "Batch integration embed",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/batch_integration_embed/index.html#quality-control",
    "href": "results/batch_integration_embed/index.html#quality-control",
    "title": "Batch integration embed",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Raw data \n    Percentage of missing results per metric \n    kBET \n    0 \n    1.000 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_full_scaled \n    0 \n    0.333 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_hvg_scaled \n    0 \n    0.333 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.176 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    immune_batch \n    0 \n    0.176 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_batch \n    0 \n    0.176 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    batch_random_integration \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    celltype_random_embedding \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    celltype_random_integration \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_full_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_hvg_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_embed_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_full_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_hvg_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    harmony_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liger_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liger_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_full_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_hvg_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    no_integration \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_integration \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scalex_full \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scalex_hvg \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_full_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_hvg_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_embed_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_full_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_hvg_scaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanvi_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanvi_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scvi_full_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scvi_hvg_unscaled \n    0 \n    0.167 \n    0.1 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    isolated_labels_sil \n    -1 \n    3.912 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    cc_score \n    -1 \n    1.268 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    silhouette_batch \n    -1 \n    1.013 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    pcr \n    -1 \n    1.001 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    silhouette \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    pcr \n    0 \n    0.054 \n    0.1 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    cc_score \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    kBET \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    pcr \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    silhouette \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    kBET \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    silhouette_batch \n    -1 \n    -0.126 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    isolated_labels_sil \n    -1 \n    -0.711 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    444 \n    444.000 \n    444.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    cc_score \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    isolated_labels_sil \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    silhouette \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    silhouette_batch \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/batch_integration_embed/index.html#raw-data",
    "href": "results/batch_integration_embed/index.html#raw-data",
    "title": "Batch integration embed",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/index.html",
    "href": "results/index.html",
    "title": "Results",
    "section": "",
    "text": "Batch integration embed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch integration feature\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBatch integration graph\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell-Cell Communication Inference (Ligand-Target)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCell-Cell Communication Inference (Source-Target)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDenoising\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimensionality reduction for visualisation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLabel Projection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultimodal Data Integration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegulatory effect prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Decomposition\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "results/batch_integration_feature/index.html",
    "href": "results/batch_integration_feature/index.html",
    "title": "Batch integration feature",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/batch_integration_feature/index.html#compute-scaling-factors",
    "href": "results/batch_integration_feature/index.html#compute-scaling-factors",
    "title": "Batch integration feature",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/batch_integration_feature/index.html#overview",
    "href": "results/batch_integration_feature/index.html#overview",
    "title": "Batch integration feature",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/batch_integration_feature/index.html#quality-control",
    "href": "results/batch_integration_feature/index.html#quality-control",
    "title": "Batch integration feature",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    hvg_conservation \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    hvg_conservation \n    -1 \n    -0.898 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    44 \n    44.000 \n    44.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    immune_batch \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_batch \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    batch_random_integration \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    celltype_random_integration \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_full_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_full_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_hvg_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    combat_hvg_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_full_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_full_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_hvg_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    fastmnn_feature_hvg_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_full_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_full_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_hvg_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    mnn_hvg_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    no_integration \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_integration \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scalex_full \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scalex_hvg \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_full_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_full_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_hvg_scaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    scanorama_feature_hvg_unscaled \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    hvg_conservation \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/batch_integration_feature/index.html#raw-data",
    "href": "results/batch_integration_feature/index.html#raw-data",
    "title": "Batch integration feature",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/dimensionality_reduction/index.html",
    "href": "results/dimensionality_reduction/index.html",
    "title": "Dimensionality reduction for visualisation",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/dimensionality_reduction/index.html#compute-scaling-factors",
    "href": "results/dimensionality_reduction/index.html#compute-scaling-factors",
    "title": "Dimensionality reduction for visualisation",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/dimensionality_reduction/index.html#overview",
    "href": "results/dimensionality_reduction/index.html#overview",
    "title": "Dimensionality reduction for visualisation",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/dimensionality_reduction/index.html#quality-control",
    "href": "results/dimensionality_reduction/index.html#quality-control",
    "title": "Dimensionality reduction for visualisation",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    qlocal \n    -1 \n    64.362 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    qnn_auc \n    -1 \n    44.443 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    continuity \n    -1 \n    37.684 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    qglobal \n    -1 \n    13.360 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    lcmc \n    -1 \n    5.286 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    qnn \n    -1 \n    5.286 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    qlocal \n    -1 \n    -3.719 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    rmse \n    -1 \n    1.180 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    density_preservation \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    trustworthiness \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    continuity \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    lcmc \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    qnn \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    qnn_auc \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    trustworthiness \n    -1 \n    0.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    rmse \n    -1 \n    -0.004 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    qglobal \n    -1 \n    -0.068 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    density_preservation \n    -1 \n    -0.411 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    324 \n    324.000 \n    324.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    mouse_hspc_nestorowa2016 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    olsson_2016_mouse_blood \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    tenx_5k_pbmc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    densmap_logCPM_1kHVG \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    densmap_pca_logCPM_1kHVG \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    high_dim_pca \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    neuralee_default \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    neuralee_logCPM_1kHVG \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    pca_logCPM_1kHVG \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    phate_default \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    phate_logCPM_1kHVG \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    phate_sqrt \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_features \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    tsne_logCPM_1kHVG \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    umap_logCPM_1kHVG \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    continuity \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    density_preservation \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    lcmc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    qglobal \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    qlocal \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    qnn \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    qnn_auc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    rmse \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    trustworthiness \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/dimensionality_reduction/index.html#raw-data",
    "href": "results/dimensionality_reduction/index.html#raw-data",
    "title": "Dimensionality reduction for visualisation",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/cell_cell_communication_source_target/index.html",
    "href": "results/cell_cell_communication_source_target/index.html",
    "title": "Cell-Cell Communication Inference (Source-Target)",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/cell_cell_communication_source_target/index.html#compute-scaling-factors",
    "href": "results/cell_cell_communication_source_target/index.html#compute-scaling-factors",
    "title": "Cell-Cell Communication Inference (Source-Target)",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/cell_cell_communication_source_target/index.html#overview",
    "href": "results/cell_cell_communication_source_target/index.html#overview",
    "title": "Cell-Cell Communication Inference (Source-Target)",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/cell_cell_communication_source_target/index.html#quality-control",
    "href": "results/cell_cell_communication_source_target/index.html#quality-control",
    "title": "Cell-Cell Communication Inference (Source-Target)",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    auprc \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    odds_ratio \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    auprc \n    -1 \n    -0.050 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    odds_ratio \n    -1 \n    -0.792 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    28 \n    28.000 \n    28.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    mouse_brain_atlas \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cellphonedb_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cellphonedb_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    connectome_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    connectome_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liana_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    liana_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    logfc_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    logfc_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    natmi_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    natmi_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_events \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    sca_max \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    sca_sum \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    true_events \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    auprc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    odds_ratio \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/cell_cell_communication_source_target/index.html#raw-data",
    "href": "results/cell_cell_communication_source_target/index.html#raw-data",
    "title": "Cell-Cell Communication Inference (Source-Target)",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/regulatory_effect_prediction/index.html",
    "href": "results/regulatory_effect_prediction/index.html",
    "title": "Regulatory effect prediction",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/regulatory_effect_prediction/index.html#compute-scaling-factors",
    "href": "results/regulatory_effect_prediction/index.html#compute-scaling-factors",
    "title": "Regulatory effect prediction",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/regulatory_effect_prediction/index.html#overview",
    "href": "results/regulatory_effect_prediction/index.html#overview",
    "title": "Regulatory effect prediction",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/regulatory_effect_prediction/index.html#quality-control",
    "href": "results/regulatory_effect_prediction/index.html#quality-control",
    "title": "Regulatory effect prediction",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Lower bound check scores after scaling \n    pearson_correlation \n    -1 \n    -3.331 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    pearson_correlation \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    spearman_correlation \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    spearman_correlation \n    -1 \n    -0.157 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    6 \n    6.000 \n    6.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    scicar_mouse_kidney \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    beta \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_scores \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    true_scores \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    pearson_correlation \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    spearman_correlation \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/regulatory_effect_prediction/index.html#raw-data",
    "href": "results/regulatory_effect_prediction/index.html#raw-data",
    "title": "Regulatory effect prediction",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/denoising/index.html",
    "href": "results/denoising/index.html",
    "title": "Denoising",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/denoising/index.html#compute-scaling-factors",
    "href": "results/denoising/index.html#compute-scaling-factors",
    "title": "Denoising",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/denoising/index.html#overview",
    "href": "results/denoising/index.html#overview",
    "title": "Denoising",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/denoising/index.html#quality-control",
    "href": "results/denoising/index.html#quality-control",
    "title": "Denoising",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Lower bound check scores after scaling \n    poisson \n    -1 \n    -10.298 \n    2.0 \n    ✗ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    mse \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Upper bound check scores after scaling \n    poisson \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    mse \n    -1 \n    -0.041 \n    2.0 \n    ✓ \n  \n  \n    Raw data \n    Long table size \n     \n    48 \n    48.000 \n    48.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pbmc \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    tabula_muris_senis_lung_random \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    alra \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    dca \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    knn_naive \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    knn_smoothing \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    magic \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    magic_approx \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    no_denoising \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    perfect_denoising \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    mse \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    poisson \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/denoising/index.html#raw-data",
    "href": "results/denoising/index.html#raw-data",
    "title": "Denoising",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "results/spatial_decomposition/index.html",
    "href": "results/spatial_decomposition/index.html",
    "title": "Spatial Decomposition",
    "section": "",
    "text": "Build a full crossing to make sure no results are missing. It’s likely some of the methods didn’t finish running on all datasets.\n\ncross_df <- crossing(\n  dataset_info %>% select(dataset_id),\n  method_info %>% select(method_id, is_baseline),\n  metric_info %>% select(metric_id)\n)\n\nTransform the results into a long format and join with the crossing.\n\nresults_long <- \n  results %>%\n  gather(metric_id, value, !!metric_info$metric_id) %>%\n  select(method_id, dataset_id, metric_id, value, is_baseline) %>%\n  full_join(cross_df, by = colnames(cross_df))\n\nPlot the raw scores.\n\nggplot(results_long) +\n  geom_point(aes(value, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/spatial_decomposition/index.html#compute-scaling-factors",
    "href": "results/spatial_decomposition/index.html#compute-scaling-factors",
    "title": "Spatial Decomposition",
    "section": "Compute scaling factors",
    "text": "Compute scaling factors\n\nCompute the minimum and maximum scores of baseline methods per dataset per metric.\nRescale values\n\n\nscaling_factors <- \n  results %>%\n    filter(is_baseline) %>%\n    gather(metric_id, value, !!metric_info$metric_id) %>%\n    group_by(dataset_id, metric_id) %>%\n    summarise(\n      scale_min = ifelse(sum(!is.na(value)) == 0, 0, min(value, na.rm = TRUE)),\n      scale_max = ifelse(sum(!is.na(value)) == 0, 1, max(value, na.rm = TRUE)),\n      .groups = \"drop\"\n    )\n\nVisualise the scaling factors.\n\n\n\n\nresults_long_scaled <- results_long %>%\n  left_join(scaling_factors, by = c(\"dataset_id\", \"metric_id\")) %>%\n  left_join(metric_info %>% select(metric_id, maximize), by = \"metric_id\") %>%\n  mutate(\n    scaled_score = case_when(\n      !is.na(value) ~ value,\n      maximize ~ scale_min,\n      !maximize ~ scale_max\n    ),\n    scaled_score = (scaled_score - scale_min) / (scale_max - scale_min),\n    scaled_score = ifelse(maximize, scaled_score, 1 - scaled_score)\n  )\n\n\n\n\n\noverall_ranking <- results_long_scaled %>%\n  group_by(method_id) %>%\n  summarise(mean_score = mean(scaled_score)) %>%\n  arrange(desc(mean_score))\n\nView results\n\n# order by ranking\nresults_long_scaled$method_id <- factor(results_long_scaled$method_id, levels = rev(overall_ranking$method_id))\n\nggplot(results_long_scaled %>% arrange(method_id)) +\n  geom_vline(aes(xintercept = x), tibble(x = c(0, 1)), linetype = \"dashed\", alpha = .5, colour = \"red\") +\n  geom_path(aes(scaled_score, method_id, group = dataset_id), alpha = .25) +\n  geom_point(aes(scaled_score, method_id, colour = is_baseline)) +\n  facet_wrap(~metric_id, ncol = 1, scales = \"free\") +\n  theme_bw() +\n  labs(x = NULL, y = NULL)"
  },
  {
    "objectID": "results/spatial_decomposition/index.html#overview",
    "href": "results/spatial_decomposition/index.html#overview",
    "title": "Spatial Decomposition",
    "section": "Overview",
    "text": "Overview\nAdd extra columns\n\nper_dataset <- results_long_scaled %>%\n  group_by(method_id, dataset_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(dataset_id = paste0(\"dataset_\", dataset_id)) %>%\n  spread(dataset_id, score)\nper_metric <- results_long_scaled %>%\n  group_by(method_id, metric_id) %>%\n  summarise(score = mean(scaled_score), .groups = \"drop\") %>%\n  mutate(metric_id = paste0(\"metric_\", metric_id)) %>%\n  spread(metric_id, score)\n\nsummary <- \n  method_info %>%\n  transmute(\n    method_id,\n    name_tmp = method_name,\n    method_name = gsub(\" \\\\(.*\", \"\", name_tmp),\n    method_config = gsub(\"[^\\\\(]*\\\\(?([^\\\\)]*)\\\\)?\", \"\\\\1\", name_tmp),\n    method_is_baseline = ifelse(is_baseline, \"yes\", \"\")\n  ) %>%\n  select(-name_tmp) %>%\n  left_join(overall_ranking, by = \"method_id\") %>%\n  left_join(per_dataset, by = \"method_id\") %>%\n  left_join(per_metric, by = \"method_id\") %>%\n  arrange(desc(mean_score))\n\n\n# fix funkyheatmap defaults so we don't need to do the processing below\ncolumn_info <- tibble(\n  id = colnames(summary)[-1],\n  name = id %>%\n    gsub(\"^[^_]+_\", \"\", .) %>%\n    gsub(\"_\", \" \", .) %>%\n    str_to_title(),\n  group = gsub(\"_.*\", \"\", id),\n  geom = case_when(\n    group == \"method\" ~ \"text\",\n    group == \"mean\" ~ \"bar\",\n    group %in% c(\"dataset\", \"metric\") ~ \"funkyrect\"\n  ),\n  palette = ifelse(group %in% c(\"mean\", \"dataset\", \"metric\"), group, NA_character_),\n  options = map2(id, geom, function(id, geom) {\n    if (id == \"method_name\") {\n      list(width = 6, hjust = 0)\n    } else if (id == \"method_config\") {\n      list(width = 4)\n    } else if (id == \"is_baseline\") {\n      list(width = 1)\n    } else if (geom == \"bar\") {\n      list(width = 4)\n    } else {\n      list()\n    }\n  })\n)\n\ng <- funky_heatmap(\n  data = summary,\n  column_info = column_info,\n  expand = c(xmax = 3),\n  col_annot_offset = 4\n)\n\nℹ Could not find column 'id' in data. Using rownames as 'id'.\n\n\nℹ No row info was provided, assuming all rows in `data` are to be plotted.\n\n\nℹ Row info did not contain group information, assuming rows are ungrouped.\n\n\nℹ No column groups was provided, deriving from column info.\n\n\nℹ Column groups did not contain a column called 'palette'. Assuming no colour scales need to be used.\n\n\nℹ Column groups did not contain a column called 'level1'. Using `column_info$group` as a makeshift column group name.\n\n\nℹ No palettes were provided, trying to automatically assign palettes.\n\n\nℹ Palette named 'mean' was not defined. Assuming palette is numerical. Automatically selected palette 'Blues'.\n\n\nℹ Palette named 'dataset' was not defined. Assuming palette is numerical. Automatically selected palette 'Reds'.\n\n\nℹ Palette named 'metric' was not defined. Assuming palette is numerical. Automatically selected palette 'YlOrBr'.\n\n\nWarning: Ignoring unknown parameters: size\n\n\n\ng"
  },
  {
    "objectID": "results/spatial_decomposition/index.html#quality-control",
    "href": "results/spatial_decomposition/index.html#quality-control",
    "title": "Spatial Decomposition",
    "section": "Quality control",
    "text": "Quality control\n\n\n\n\n \n  \n    section \n    name \n    variable \n    lower \n    value \n    upper \n    test \n  \n \n\n  \n    Metric scaling \n    Upper bound check scores after scaling \n    r2 \n    -1 \n    1.000 \n    2.0 \n    ✓ \n  \n  \n    Metric scaling \n    Lower bound check scores after scaling \n    r2 \n    -1 \n    -2.578 \n    2.0 \n    ✗ \n  \n  \n    Raw data \n    Long table size \n     \n    98 \n    98.000 \n    98.0 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results \n     \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    destvi \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_alpha_0_5 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_alpha_1 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    pancreas_alpha_5 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    tabula_muris_senis_alpha_0_5 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    tabula_muris_senis_alpha_1 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per dataset \n    tabula_muris_senis_alpha_5 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cell2location_amortised_detection_alpha_20 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cell2location_detection_alpha_20 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cell2location_detection_alpha_20_nb \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    cell2location_detection_alpha_200 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    destvi \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    nmf \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    nmfreg \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    nnls_scipy \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    random_proportions \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    rctd \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    seuratv3 \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    stereoscope \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    tangram \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per method \n    true_proportions \n    0 \n    0.000 \n    0.1 \n    ✓ \n  \n  \n    Raw data \n    Percentage of missing results per metric \n    r2 \n    0 \n    0.000 \n    0.1 \n    ✓"
  },
  {
    "objectID": "results/spatial_decomposition/index.html#raw-data",
    "href": "results/spatial_decomposition/index.html#raw-data",
    "title": "Spatial Decomposition",
    "section": "Raw data",
    "text": "Raw data\n\n\nMethodsMetricsDatasetsResultsScaling factorsSummaryQuality control\n\n\n\nInputs.table(method_info)\n\n\n\n\n\n\n\n\n\nInputs.table(metric_info)\n\n\n\n\n\n\n\n\n\nInputs.table(dataset_info)\n\n\n\n\n\n\n\n\n\nInputs.table(results)\n\n\n\n\n\n\n\n\n\nInputs.table(scaling_factors)\n\n\n\n\n\n\n\n\n\nInputs.table(summary)\n\n\n\n\n\n\n\n\n\nInputs.table(qc)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmethod_info = transpose(method_info_t)\nmetric_info = transpose(metric_info_t)\ndataset_info = transpose(dataset_info_t)\nresults = transpose(results_t)\nscaling_factors = transpose(scaling_factors_t)\nsummary = transpose(summary_t)\nqc = transpose(qc_t)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenProblems NBT2022 reproducibility",
    "section": "",
    "text": "An experimental repository for producing the results from the NBT 2022 paper."
  }
]