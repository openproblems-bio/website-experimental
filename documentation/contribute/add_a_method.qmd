---
title: Add a method
order: 40
---

This page describes how to add a method to an already existing task. Make sure you have followed the ["Requirements"](requirements.qmd) and ["Getting started"](getting_started.qmd) guides.

There are 2 type of components you can add:

* `control method`:  The control methods are methods that form a baseline (random/ground truth) to compare the methods from the same task against.
* `method`: scripts to be benchmarked on how to perform the task.

You can find the components in their respective task directories (`src/task_name/`) namely the `control_methods` and `methods` directory. The setup of both components is similar with small differences in the config yaml. 
The differences will be pointed out below. whenever `methods` is mentioned it should be replaced with `control_methods` when adding a control method.

## Directory

To add a method, create a new directory in the `src/task/methods` (where `task` is replaced by the task name. e.g. `denoising`) with the name of the new method in snake case. A task can also contain a subtask then this should be `src/task/methods_subtask/`.
Add a `config.vsh.yaml` and a script file e.g. `script.py` or `script.R`. You can also add additional helper files to this `dir` if it is required for the method e.g. method specific unit test `test.py`

structure of the new method directory:

    src/task/methods/new_method
        ├── script.py/R                  method script
        ├── config.vsh.yaml              config file for method
        └── additional files             Helper files like e.g. tsv file, unit test specific for method, ...

## config.vsh.yaml

Full documentation on the Viash configuration file is available on the [Viash documentation site](https://viash.io/reference/).

### Merge
```yaml
__merge__: ../../api/comp_method.yaml
```
This file contains metadata that is needed for all the methods. It will contain the required arguments such as the `--input` files and the `--output` files

### Functionality

This section of the configuration file contains information about the metadata of the script including script specific parameters and a list of resource files. 

```yaml
functionality:
  # a unique name for your method, same as what is being output by the script.
  # must match the regex [a-z][a-z0-9_]*
  name: method
  namespace: task_name/methods
  # metadata for your method
  description: A description for your method.
  info:
    type: method
    method_name: Method
    preferred_normalization:
    variants:


  # component parameters
  arguments:
    # Method-specific parameters. 
    # Change these to expose parameters of your method to Nextflow (optional)
    - name: "--n_neighbors"
      type: "integer"
      default: 5
      description: Number of neighbors to use.

  # files your script needs
  resources:
    # the script itself
    - type: python_script
      path: script.py
    # additional resources your script needs (optional)
    - path: pretrained_model.pt
```

In this section of the configuration you should focus on updating the following sections:

1. Description and Info - Information about the method

    * `namespace`: this should be structured according to `task_name/methods` or `task_name/control_methods`. If the task contains subtasks this should be `task/subtask_name_methods`, the same for control methods. Where `task_name` is replaced by the task name.
    * `info/type`: For a method this should be `method`. For control_method it should be `negative_control` or `positive_control`. 

2. Arguments - Each section here defines a command-line argument for the script. These sections are all passed to the script in the form of a dictionary called `par`. You only need to change the method-specific parameters, and if you would like these parameters hard-coded into the script, you do not need to provide any parameters here. 
3. Resources - This section describes the files that need to be included in your component. For example if you'd like to add a file containing model weights called `weights.pt`, add `{ type: file, path: weights.pt }` to the resources. You can now load the additional resource in your script by using at the path `meta['resources_dir'] + '/weights.pt'`.


### Platform

The Platform section defines the information about how the Viash component is run on various backend platforms.

1. Docker ([docs](https://viash.io/guide))
2. Nextflow ([docs](https://viash.io/guide))

::: {.panel-tabset}

## Python
```yaml
# target platforms
platforms:

  # By specifying 'docker' platform, viash will build a standalone
  # executable which uses docker in the back end to run your method.
  - type: docker
    # you need to specify a base image that contains at least bash and python
    image: python:3.10
    # You can specify additional dependencies with 'setup'.
    # See https://viash.io/reference
    # for more information on how to add more dependencies.
    setup:
      # - type: apt
      #   packages:
      #     - bash
      - type: python
        pip:
          - pyyaml
          - anndata>=0.8

  # By specifying a 'nextflow', viash will also build a viash module
  # which uses the docker container built above to also be able to
  # run your method as part of a nextflow pipeline.
  - type: nextflow
    directives:
      label: [ lowmem, lowcpu ]
```

## R
```yaml
# target platforms
platforms:

  # By specifying 'docker' platform, viash will build a standalone
  # executable which uses docker in the back end to run your method.
  - type: docker
    # you need to specify a base image that contains at least bash and python
    image: eddelbuettel/r2u:22.04
    # You can specify additional dependencies with 'setup'.
    # See https://viash.io/reference
    # for more information on how to add more dependencies.
    setup:
      # - type: apt
      #   packages:
      #     - bash
      - type: apt
        packages: [ libhdf5-dev, libgeos-dev, python3, python3-pip, python3-dev, python-is-python3, git ]
      - type: python
        pip: [ anndata>=0.8, pyyaml ]
      - type: r
        cran: [ anndata]

  # By specifying a 'nextflow', viash will also build a viash module
  # which uses the docker container built above to also be able to
  # run your method as part of a nextflow pipeline.
  - type: nextflow
    directives:
      label: [ lowmem, lowcpu ]
```
:::

The most important part of this section to update is the `setup` definition that describes the packages that need to be installed in the docker container for the method to run. There are many different methods for specifying these requirements described in the Viash [docs](https://viash.io/reference). It is required to add the python setup to include the `pyyaml` package due to general unit testing done (if not already in the image). When creating an Rscript also add the `anndata>=0.8` package int the python setup.

You can also change the memory and CPU utilization be editing the Nextflow labels section. Available options are `[low|med|high]` for each of `mem` and `cpu`. The corresponding resource values can be found in the `/src/wf_utils/labels.config` file.

:::{.callout-note}
**Tip:** After making changes to the components dependencies, you will need to rebuild the docker container as follows:

```sh
$ viash run -- ---setup cachedbuild
[notice] Running 'docker build -t method:dev /tmp/viashsetupdocker-method-tEX78c'
```
:::

:::{.callout-note}
**Tip #2:** You can view the dockerfile that Viash generates from the config file using the `---dockerfile` argument:
```sh
$ viash run -- ---dockerfile
```
:::

## script file

The script has three main sections: Imports/libraries, Viash block, and Method.

### Imports

This section defines which packages the method expects, if you want to import a new different package, add the `import` statement here **and** add the dependency to `config.vsh.yaml` (see above).

::: {.panel-tabset}

### Python
```python
import anndata as ad
```

### R
```R
library(anndata, warn.conflicts = FALSE)
```
:::


### Viash block

This optional code block exists to facilitate prototyping so your script can run when called directly by running `python script.py` (or `Rscript script.R` for R users). 

::: {.panel-tabset}
## Python
```python
## VIASH START
# Anything within this block will be removed by `viash` and will be
# replaced with the parameters as specified in your config.vsh.yaml.
par = {
    # Required arguments for the task
    'input_train': 'train.h5ad',
    'input_test': '.test.h5ad',
    'output': 'output.h5ad',
    # Optional method-specific arguments
    'n_neighbors': 5,
}
meta = { 
  'functionality_name': 'foo' 
}
## VIASH END
```

## R
```R
## VIASH START
# Anything within this block will be removed by `viash` and will be
# replaced with the parameters as specified in your config.vsh.yaml.
par <- list(
    # Required arguments for the task
    input_train= 'train.h5ad',
    input_test= 'test_mod1.h5ad',
    output= 'output.h5ad',
    # Optional method-specific arguments
    n_neighbors= 5,
)
meta <- list (
  functionality_name= 'foo' 
)
## VIASH END
```
:::

Here, the `par` dictionary contains all the `arguments` defined in the `config.vsh.yaml` file. 

### Method

This code block will typically consist of reading the input files, performing some preprocessing, training a model on the train cells, generating predictions for the test cells, and outputting the predictions as an AnnData file.

::: {.panel-tabset}
## Python
```python
## Data reader
print('Reading input files', flush=True)

input_train = ad.read_h5ad(par['input_train_mod1'])
input_test = ad.read_h5ad(par['input_test_mod1'])

print('processing Data', flush=True)
# ... preprocessing ... 
# ... train model ...
# ... generate predictions ...

# write output to file
adata = ad.AnnData(
    X=y_pred,
    uns={
        'dataset_id': input_train.uns['dataset_id'],
        'method_id': meta['functionality_name'],
    },
)

print('writing to output files', flush=True)
adata.write_h5ad(par['output'], compress='gzip')
```
## R
```R

```
:::

Depending on the task The output is stored in different locations in the anndata. e.g. for `denoising` it is located in the `.layers['denoised']` and for `dimensionality_reduction` it is stored in `.obsm[X_emb]`.

## API

For in depth documentation see [API file formats](../create_task/api_file_formats.qmd) and [API components specs](../create_task/api_component_specs.qmd).

In the API directory there are yaml files that have info about the anndata objects. These files all start with `anndata_*.yaml`.

When developing a method it can be useful to check these files on what the anndata objects consist of. For instance the `anndata_dataset.yaml` has information on what is required that the datasets has.

If a new field needs to be added...


## Nextflow

After developing your method you add it to the nextflow workflow that can be found at `task_name/workflows/run/main.nf`

Execute the cmd below to create and build the docker and nextflow filepaths.

```bash
viash ns build -q task_name --paralell --setup cachedbuild
```

```nextflow
// import control methods
include { true_labels } from "$targetDir/label_projection/control_methods/true_labels/main.nf"
include { majority_vote } from "$targetDir/label_projection/control_methods/majority_vote/main.nf"
include { random_labels } from "$targetDir/label_projection/control_methods/random_labels/main.nf"

// import methods
include { knn } from "$targetDir/label_projection/methods/knn/main.nf"
include { mlp } from "$targetDir/label_projection/methods/mlp/main.nf"
include { logistic_regression } from "$targetDir/label_projection/methods/logistic_regression/main.nf"
include { scanvi } from "$targetDir/label_projection/methods/scanvi/main.nf"
include { seurat_transferdata } from "$targetDir/label_projection/methods/seurat_transferdata/main.nf"
include { xgboost } from "$targetDir/label_projection/methods/xgboost/main.nf"
```

Also add your method further down the file with the include name you have given.

```nextflow
// construct a map of methods (id -> method_module)
methods = [ true_labels, majority_vote, random_labels, knn, mlp, logistic_regression, scanvi, seurat_transferdata, xgboost ]
  .collectEntries{method ->
    [method.config.functionality.name, method]
  }
```

## testing

Check out the in depth documentation [here](run_tests.qmd)

### unit test

You can test your method by using the following command:

```bash
viash test path/to/method/config.vsh.yaml
```

There is a general unit test that you can find in the `comp_method.yaml` that will be executed. If you added a specific unit test for your method it will also be executed if added correctly.

Depending on the result you will get a notification on how many tests succeeded or failed:

```bash
SUCCESS! All 1 out of 1 test scripts succeeded!
```

### Workflow test

testing of the full workflow can be done by using the following command:

`task_name/workflows/run/run_test.sh`

## Final steps

Add yourself to the `task_name/api/authors.yaml` file.

When you are finished with your component create a Pull Request according to the instructions [here](create_pull_request.qmd).